{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0e66b9f-564b-4990-8074-ee5c52a24ebd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Installing Necessary Modules"
    }
   },
   "outputs": [],
   "source": [
    "#Installing Necessary Modules\n",
    "%pip install sentence-transformers faiss-cpu openai langchain python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4df7a2fc-43a6-4689-a3a3-e49aa8c1e3e2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reading Documents From File"
    }
   },
   "outputs": [],
   "source": [
    "#Reading Documents From File\n",
    "from docx import Document\n",
    "def read_docx(file_path):\n",
    "    \"\"\"Extract text from a .docx file.\"\"\"\n",
    "    doc = Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return \"\\n\".join(full_text)\n",
    "\n",
    "# Set the folder path where your .docx files are stored\n",
    "folder_path = \"/Workspace/Users/varlock4444@gmail.com/Documents\"\n",
    "documents = []   # This list will hold the text from each document\n",
    "doc_names = []   # Optional: to track file names\n",
    "files = os.listdir(folder_path)\n",
    "print(\"Files in the foblder:\", files)\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        text = read_docx(file_path)\n",
    "        documents.append(text)\n",
    "        doc_names.append(filename)\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17810338-86e1-4516-a594-e52fe4a746b6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Calling Open AI ADA Embedding Model"
    }
   },
   "outputs": [],
   "source": [
    "#Calling Open AI ADA Embedding Model\n",
    "from openai import AzureOpenAI\n",
    "import numpy as np\n",
    "\n",
    "endpoint=\"https://rXXXX.openai.azure.com/\"\n",
    "subscription_key=\"XXXX\" \n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    "    api_version=\"2023-05-15\",\n",
    ")\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    \"\"\"Generate an embedding for the given text using OpenAI's ada-002 model.\"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=model\n",
    "    )\n",
    "   \n",
    "    embedding = response.data[0].embedding\n",
    "    return embedding\n",
    "\n",
    "# Generate embeddings for all documents\n",
    "document_embeddings = []\n",
    "for doc in documents:\n",
    "    emb = get_embedding(doc)\n",
    "    document_embeddings.append(emb)\n",
    "\n",
    "# Convert to a NumPy array with dtype float32 (required by FAISS)\n",
    "document_embeddings = np.array(document_embeddings).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f41a076-2b61-4c17-aa74-6dd9bf6139e7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Indexing the Embeddings Using FAISS"
    }
   },
   "outputs": [],
   "source": [
    "# Indexing the Embeddings Using FAISS\n",
    "import faiss\n",
    "# Determine the dimensionality from one of the embeddings\n",
    "embedding_dimension = document_embeddings.shape[1]\n",
    "# Create a FAISS index (using L2 distance)\n",
    "index = faiss.IndexFlatL2(embedding_dimension)\n",
    "# Add the document embeddings to the index\n",
    "index.add(document_embeddings)\n",
    "print(f\"Number of documents indexed: {index.ntotal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01664105-1da6-4c0a-9ece-e3046dbb4682",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Building the Retrieval Pipeline"
    }
   },
   "outputs": [],
   "source": [
    "#Building the Retrieval Pipeline\n",
    "import numpy as np\n",
    "def retrieve_documents(query,k):\n",
    "    \"\"\"Retrieve the top-k relevant documents for the given query.\"\"\"\n",
    "    # Embed the query text using the same ada-002 model\n",
    "    query_embedding = np.array([get_embedding(query)]).astype(\"float32\")\n",
    "    \n",
    "    # Search the FAISS index for the nearest neighbors\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    # For each retrieved index, create a dictionary with both document text and its name\n",
    "    retrieved_docs = [\n",
    "        {\"doc_text\": documents[i], \"doc_name\": doc_names[i]} \n",
    "        for i in indices[0]\n",
    "    ]\n",
    "    return retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d27dc197-268a-4ede-a0b1-ad670109daad",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generating an Answer with a Generative Model"
    }
   },
   "outputs": [],
   "source": [
    "#Generating an Answer with a Generative Model\n",
    "\n",
    "def generate_answer(query, context_docs):\n",
    "\n",
    "    client = AzureOpenAI(\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    "    api_version=\"2024-08-01-preview\"\n",
    ")\n",
    "\n",
    "    \"\"\"Generate an answer using the retrieved context and the user query.\"\"\"\n",
    "    # Combine the context documents into a single string with document names as reference.\n",
    "    # For each retrieved document, include a header with the document name followed by its text.\n",
    "    context_str = \"\\n\\n\".join(\n",
    "        [f\"Document: {doc['doc_name']}\\n{doc['doc_text']}\" for doc in context_docs]\n",
    "    )\n",
    "    \n",
    "    chat_prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"You are a knowledgeable assistant. Based on the following context, answer the question. Context: \" + context_str\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": query\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    \n",
    "    messages = chat_prompt\n",
    "    # Generate the completion\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95\n",
    "    )\n",
    "    \n",
    "    completion_choices = completion.__getattribute__('choices')\n",
    "    # Access the first choice and its message\n",
    "    first_choice = completion_choices[0]\n",
    "    answer = first_choice.message.content \n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21340e52-f5b6-4882-90c0-9457ae89de6e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "End User Query And Function Calls"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "user_query=\"how much will it cost to create rag model with 30 page document also mention what technologies are included in the SRS document\"\n",
    "# Retrieve the top relevant documents\n",
    "retrieved_context = retrieve_documents(user_query, k=len(documents))\n",
    "# Generate an answer using the retrieved context\n",
    "answer = generate_answer(user_query, retrieved_context)\n",
    "print(answer)\n",
    "doc_name=[]\n",
    "print(\"\\nReferenced Documents:\")\n",
    "for doc in retrieved_context:\n",
    "    doc_name.append(doc[\"doc_name\"])\n",
    "print(doc_name)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "RAG System",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
